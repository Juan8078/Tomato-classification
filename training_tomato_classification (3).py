# -*- coding: utf-8 -*-
"""training_tomato_classification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AU0MZDWZcFkD_hhxy3tXqUfmC1LFWVaH
"""

import tensorflow as tf
print("GPU Available: ", tf.config.list_physical_devices('GPU'))

"""###Setup Dataset"""

from google.colab import drive
import os

drive.mount('/content/drive')

base_dir = "/content/drive/My Drive/Colab Notebooks/"
!ls "/content/drive/My Drive/Colab Notebooks/"

training_dir = os.path.join(base_dir, "projects/tomato_project/datasets/version_1/training")
validation_dir = os.path.join(base_dir, "projects/tomato_project/datasets/version_1/validation")

train_damaged = os.path.join(training_dir, 'damaged/')
train_old = os.path.join(training_dir, 'old/')
train_ripe = os.path.join(training_dir, 'ripe/')
train_unripe = os.path.join(training_dir, 'unripe/')
print("Count init train tomato")
print("count of image damaged : ", len(os.listdir(train_damaged)))
print("count of image old : ", len(os.listdir(train_old)))
print("count of image ripe : ", len(os.listdir(train_ripe)))
print("count of image unripe : ", len(os.listdir(train_unripe)))

print("\n")

validation_damaged = os.path.join(validation_dir, 'damaged/')
validation_old = os.path.join(validation_dir, 'old/')
validation_ripe = os.path.join(validation_dir, 'ripe/')
validation_unripe = os.path.join(validation_dir, 'unripe/')
print("Count init validation tomato")
print("count of image damaged : ", len(os.listdir(validation_damaged)))
print("count of image old : ", len(os.listdir(validation_old)))
print("count of image ripe : ", len(os.listdir(validation_ripe)))
print("count of image unripe : ", len(os.listdir(validation_unripe)))

"""###Pre Processing"""

import tensorflow as tf
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras import regularizers

train_datagen = ImageDataGenerator(
    rescale=1./255,
    rotation_range=20,
    horizontal_flip=False,
    shear_range=0.2,
    fill_mode='nearest',
    width_shift_range=0.2,
    height_shift_range=0.2,
    zoom_range=0.1
)

valid_datagen = ImageDataGenerator(
    rescale=1./255,
    rotation_range=20,
    horizontal_flip=False,
    shear_range=0.2,
    fill_mode='nearest',
    width_shift_range=0.2,
    height_shift_range=0.2,
    zoom_range=0.1
)

train_generator = train_datagen.flow_from_directory(
    training_dir,
    batch_size = 32,
    target_size=(256,256),
    class_mode="categorical",
    seed=46
)

validation_generator = valid_datagen.flow_from_directory(
    validation_dir,
    batch_size = 32,
    target_size=(256,256),
    class_mode="categorical",
    seed=46
)

label_map=(train_generator.class_indices)
print(label_map)

# optional untuk batas jika 99% stop pelatihan
class myCallback(tf.keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs = {}):
    if(logs.get('accuracy') > 0.99):
      print('\nAkurasi mencapai 99%')
      self.model.stop_training = True

callbacks = myCallback()

model = tf.keras.models.Sequential([
    #first layer
    tf.keras.layers.Conv2D(16, (3,3), activation='relu', padding='same', kernel_initializer='he_normal', input_shape=(256, 256, 3)),
    tf.keras.layers.MaxPooling2D(2, 2),
    #second layer
    tf.keras.layers.Conv2D(32, (3,3), activation='relu', padding='same', kernel_initializer='he_normal'), tf.keras.layers.MaxPooling2D(2,2),
    #third layer
    tf.keras.layers.Conv2D(64, (3,3), activation='relu', padding='same', kernel_initializer='he_normal'), tf.keras.layers.MaxPooling2D(2,2),
    tf.keras.layers.Conv2D(64, (3,3), activation='relu', padding='same', kernel_initializer='he_normal'), tf.keras.layers.MaxPooling2D(2,2),
    tf.keras.layers.Conv2D(64, (3,3), activation='relu', padding='same', kernel_initializer='he_normal'), tf.keras.layers.MaxPooling2D(2,2),
    #flatten dropout
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dropout(0.3),
    #fully connected layers
    tf.keras.layers.Dense(128, activity_regularizer=regularizers.l2(0.001), activation='relu'),
    tf.keras.layers.Dropout(0.6),
    tf.keras.layers.Dense(4, activation='softmax')
])

model.summary()

"""###Core Convolutional Neural Network (CNN) Training"""

adam = Adam(learning_rate=0.001)

model.compile(
    loss='categorical_crossentropy',
    optimizer=adam,
    metrics=['accuracy']
)

history = model.fit(
    train_generator,
    steps_per_epoch=len(train_generator),
    epochs=100,
    validation_data=validation_generator,
    validation_steps=len(validation_generator)
)

"""###Training Result"""

from sklearn.metrics import classification_report
test_generator = ImageDataGenerator(rescale=1./255)
test_data_generator=test_generator.flow_from_directory(
    # validation_dir,
    '/content/drive/MyDrive/Colab Notebooks/projects/tomato_project/datasets/version_1/validation',
    target_size = (256,256),
    batch_size=30,
    shuffle=False
)

label_map=(train_generator.class_indices)
print(label_map)

import tensorflow as tf
tf.keras.models.save_model(model, 'modelcnn_final.keras')

import matplotlib.pyplot as plt
acc = history.history['accuracy']
val_acc = history.history['val_accuracy']

loss = history.history['loss']
val_loss = history.history['val_loss']

plt.figure(figsize=(8, 8))
plt.subplot(2, 1, 1)
plt.plot(acc, label='training accuracy')
plt.plot(val_acc, label='validation accuracy')
plt.legend(loc='lower right')
plt.ylabel('accuracy')
plt.title('training and validation accuracy')

plt.subplot(2, 1, 2)
plt.plot(loss, label='training loss')
plt.plot(val_loss, label='validation loss')
plt.legend(loc='upper right')
plt.ylabel('cross entropy')
plt.title('training and validation loss')
plt.xlabel('epoch')
plt.show()

model.evaluate(validation_generator)

# save model
model.save('/content/drive/My Drive/Colab Notebooks/projects/tomato_project/models/6Mei5.h5')

# save model
model.save('/content/drive/My Drive/Colab Notebooks/projects/tomato_project/models/6Mei5.keras')

import sklearn
from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns

for x in label_map:
      print(label_map[x],":",x)

prediction = model.predict(test_data_generator)
y_pred=np.argmax(prediction,axis=1)

plt.figure(figsize=(8,8))
sns.heatmap(confusion_matrix(test_data_generator.classes,y_pred),annot=True)
plt.title("Confusion Matrix")
plt.ylabel("Actual")
plt.xlabel("Prediction")

label_map=(train_generator.class_indices)
print(label_map)

from sklearn.metrics import classification_report
print(classification_report(test_data_generator.classes,y_pred))

"""###Error Analysis"""

import numpy as np
from sklearn.metrics import confusion_matrix, classification_report

y_pred = model.predict(validation_generator)
y_true = validation_generator.classes

confusion_mtx = confusion_matrix(y_true, np.argmax(y_pred, axis=1))
print(confusion_mtx)

report = classification_report(y_true, np.argmax(y_pred, axis=1))
print(report)

"""###Test Real Case

"""

from google.colab import files
from keras.preprocessing import image
import numpy as np

uploaded = files.upload()

for fn in uploaded.keys():
    path = fn
    img = image.load_img(path, target_size=(256, 256))
    x = image.img_to_array(img)
    x = np.expand_dims(x, axis=0)
    x = x / 255.0

    predictions = model.predict(x)
    class_index = np.argmax(predictions, axis=1)[0]

    class_label = list(label_map.keys())[list(label_map.values()).index(class_index)]

    # Mendapatkan nilai probabilitas tertinggi
    confidence = np.max(predictions) * 100  # Mengonversi ke persentase

    # Menampilkan gambar
    plt.imshow(img)
    plt.axis('off')
    plt.show()

    print(f'Gambar {fn} termasuk ke dalam Tomat kelas {class_index}: {class_label}')
    print(f'Confidence: {confidence:.2f}%')

"""###Test Case Output G-Drive"""

import time
from google.colab import files
from keras.preprocessing import image
import numpy as np
import json

uploaded = files.upload()

for fn in uploaded.keys():
    path = fn
    img = image.load_img(path, target_size=(256, 256))
    x = image.img_to_array(img)
    x = np.expand_dims(x, axis=0)
    x = x / 255.0

    start_time = time.time()

    predictions = model.predict(x)
    class_index = np.argmax(predictions, axis=1)[0]
    class_label = list(label_map.keys())[list(label_map.values()).index(class_index)]

    end_time = time.time()
    elapsed_time = end_time - start_time
    class_index = int(class_index)

    accuracy = np.max(predictions) * 100

    data = {
        "class_index": class_index,
        "class_label": class_label,
        "accuracy": f'{accuracy:.2f}%',
        "elapsed_time": f'{elapsed_time:.4f} seconds'
    }

    json_data = json.dumps(data, indent=4)
    print(json_data)

from google.colab import drive
import json

file_path = "/content/drive/My Drive/Colab Notebooks/projects/tomato_project/output/result.json"

with open(file_path, 'w') as json_file:
    json.dump(data, json_file)

print(f'File {file_path} \n berhasil diunggah ke Google Drive.')

from keras.preprocessing import image
import numpy as np
import json

file_path = "/content/drive/MyDrive/Colab Notebooks/projects/tomato_project/input/May_01_2024_17_09_50.jpg"
img = image.load_img(file_path, target_size=(256, 256))
x = image.img_to_array(img)
x = np.expand_dims(x, axis=0)
x = x / 255.0

predictions = model.predict(x)
class_index = np.argmax(predictions, axis=1)[0]
class_label = list(label_map.keys())[list(label_map.values()).index(class_index)]

start_time = time.time()
end_time = time.time()
elapsed_time = end_time - start_time

accuracy = np.max(predictions) * 100

data = {
    "class_index": int(class_index),
    "class_label": class_label,
    "accuracy": f'{accuracy:.2f}%',
    "elapsed_time": f'{elapsed_time:.4f} seconds'
}

json_data = json.dumps(data, indent=4)
print(json_data)

from google.colab import drive
import json

file_path = "/content/drive/My Drive/Colab Notebooks/projects/tomato_project/output/result.json"

with open(file_path, 'w') as json_file:
    json.dump(data, json_file)

print(f'File {file_path} \n berhasil diunggah ke Google Drive.')

from google.colab import drive
drive.mount('/content/drive')